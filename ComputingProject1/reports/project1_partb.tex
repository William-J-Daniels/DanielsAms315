\documentclass[12pt, letterpaper]{article}

\title{AMS 315 Project 1 Part B}
\author{William Daniels}
\date{\today}


\usepackage{graphicx}
\graphicspath{{../figures/}}

\usepackage{titlesec}
\titleformat{\section}[runin]
            {\bfseries}
            {\thesection}
            {}
            {}
            [:]

\usepackage[obeyspaces,spaces]{url}

\usepackage{listings}

\usepackage{multirow}

\usepackage{float}

\usepackage{geometry}
\geometry{letterpaper, margin = 1 in}


\begin{document}
\pagenumbering{gobble}

\noindent
\begin{minipage}{0.7\textwidth}
    Will Daniels; 112774725

    \today

    Data analysis; AMS 315

    Project 1 Part A
\end{minipage}
%
\begin{minipage}{0.3\textwidth}
    \begin{flushright}
        \includegraphics[height = 48pt]{../figures/SBULogoStacked.png}
    \end{flushright}
\end{minipage}
\noindent
\rule{\textwidth}{1pt}

\section*{Introduction}
In this report, I will detail my work in recovering which transformation was applied to my dataset to make it nonlinear.

\section*{Methods}
I chose to do my analysis using an interactive python notebook because I am already very familiar with python and some of its numerical, statistical, and visualization packages.
Of them, I used numpy, pandas, matplotlib, and scipy.
I first separated the data into levels according to the distance between consecutive independent values: the data was split when two consecutive IVs were more than some threshold apart.
My python function that accomplishes this is in listing~\ref{lst:bin}.
The fit parameters and lack of fit test were also coded manually, shown in listing~\ref{lst:fit}.
I then applied the root, log, inverse, and square transformations to the independent and dependent values and ran the analysis code on each permutation with replacement of independent and dependent transformations.
A subset of these results are shown in figure~\ref{fig:perm}.
The transformation with the highest \(r^2\) value without significant lack of fit at \(\alpha=0.1\) was selected.

\section*{Results}
\begin{figure}[!b]
    \centering
    \includegraphics[height=5cm]{Rootx_Inversey_binned}
    %\hspace{2cm}
    \includegraphics[height=5cm]{rsq_vs_plof}
    \caption{On the left, the transformation that I select: root x vs inverse y. On the right, a plot of \(r^2\) vs \(p_{LOF}\) for every transformation tested.}
    \label{fig:results}
\end{figure}

The transformation with the highest \(r^2\) value without significant lack of fit at \(\alpha=0.1\) is \textbf{root x vs inverse y}, with \(r^2=0.49\) and \(p_{LOF}=0.9076\).

\section*{Conclusions and discussion}
Interestingly, as can be seen in figure~\ref{fig:results}, no transformation had significant lack of fit, so the choice came exclusively to the explanatory power of the model.

\newpage
%\section*{Appendix: figures and tables}
\textbf{Appendix: Code}

\begin{lstlisting}[language=Python, caption=Python bin data according to the didstance between consecutive independent values,  label={lst:bin}]
def bin_nearly_repeated(data, col, factor=0.5):
    """
    A function to bin nearly repeated values, as in
    x    | y         x    | y
    -----+-- becomes -----+--
    1.01 | 2         1.02 | 2
    1.02 | 3         1.02 | 3
    1.03 | 4         1.02 | 4
    Required input:
    - A pandas dataframe
    - The name of the column being binned
    Optional input:
    - The factor by which to scale the threshold; this modifies the mean of the diff and is set to 0.5 by default
    Returns:
    - A sorted pandas dataframe containing a column of binned values "x" with their corresponding values "y", equal in length to the input dataframe
    """
    data.sort_values(col, inplace=True)

    # break the relevant column into sections where consecutive points are within the threshold from each other
    dx = np.diff(data[col])

    threshold = factor*dx.mean()
    dist_threshold = dist_factor*max(dx)
    
    break_idx = np.argwhere(dx > threshold)[:,0] + 1
    chunks = np.split(np.asarray(data[col]), break_idx)
    
    # make each element of every chunk equal to the average of the chunk
    # take advantage of the fact that np.split returns views
    for chunk in chunks:
      chunk[:] = chunk.mean()
      
    return data
\end{lstlisting}

\begin{lstlisting}[language=python, caption=Python function to compute fit parameters and perform lack of fit test; also computes 0.95 confidence and prediction intervals, label={lst:fit}]
def fit_and_plot(data, xcol, ycol):
    """
    I'll be doing this many times in the analysis so I made it a function
    required input:
    - a pandas dataframe containing the x and y data
    - the string for the dataframe column with independent values
    - the string for the dataframe column with dependent values
    returns:
    - the slope
    - the intercept
    - the R^2 value
    - pvalue of lack of fit test
    - the figure object that the fit and residuals are drawn on
    """
    # useful quantities
    xbar = data[xcol].mean()
    ybar = data[ycol].mean()
    Sxx = np.sum((data[xcol] - xbar)**2)
    Sxy = np.sum((data[xcol] - xbar) * (data[ycol] - ybar))
    Syy = np.sum((data[ycol] - ybar)**2)

    # estimate fit parameters and add fit + residuals to dataframe
    B1 = Sxy / Sxx
    B0 = ybar - B1*xbar
    fitcol = xcol + ycol + "_fit"
    rescol = xcol + ycol + "_res"
    data[fitcol] = np.polyval([B1, B0], data[xcol])
    data[rescol] = data[ycol] - data[fitcol]
    SSres = np.sum(data[rescol]**2)

    # compute r, r squared, degrees of freedom, and mean squared error
    r = Sxy/np.sqrt(Sxx*Syy)
    rsq = (Syy - SSres) / Syy
    n = len(data)
    dof = n - 2
    MSE = np.sqrt(np.sum((data[ycol] - data[fitcol])**2) / dof)

    # compute .95 confidence and prediction intervals and add to dataframe
    t = stats.t.ppf(0.975, dof)
    CI = t * MSE * np.sqrt(1/n + ((data[xcol]-xbar)**2)/Sxx)
    PI = t * MSE * np.sqrt(1 + 1/n + ((data[xcol]-xbar)**2)/Sxx)

    upper_ci_col = xcol + ycol + "_upperCI"
    lower_ci_col = xcol + ycol + "_lowerCI"
    upper_pi_col = xcol + ycol + "_upperPI"
    lower_pi_col = xcol + ycol + "_lowerPI"

    data[upper_ci_col] = data[fitcol] + CI
    data[lower_ci_col] = data[fitcol] - CI
    data[upper_pi_col] = data[fitcol] + PI
    data[lower_pi_col] = data[fitcol] - PI

    # break data into levels to prepare for lack of fit test
    break_idx = np.nonzero(np.diff(data[xcol]))[0] + 1
    levels_x = np.split(np.asarray(data[xcol]), break_idx)
    levels_y = np.split(np.asarray(data[ycol]), break_idx)

    # find pure experimental error and error due to lack of fit
    SSPexp = 0
    SSPexp_dof = 0
    for ly in levels_y:
        lybar = ly.mean()
        SSPexp += np.sum((ly - lybar)**2)
        SSPexp_dof += len(ly) - 1
    SSlack = SSres - SSPexp
    SSlack_dof = n - 2 - SSPexp_dof

    # calculate the statistic and result of the lack of fit test
    F0 = (SSPexp*SSlack_dof) / (SSlack*SSPexp_dof)
    pLOF = stats.f.cdf(F0, SSPexp_dof, SSlack_dof)

    # visualize
    fig, (fax, rax) = plt.subplots(2, layout="tight",
                                   sharex=True)

    # plot the fit
    fax.set_ylabel("$"+ycol+"$")
    fax.scatter(data[xcol], data[ycol],
                marker=".", color="black")
    l = "$\\beta_1 = %.2f$\n$\\beta_0 = %.2f$\n$r^2 = %.2f$\n$r_{s} = %.2f$"\
         %(B1, B0, rsq, r)
    fax.plot(data[xcol], data[fitcol],
             color="red", label=l)
    
    # plot .95 CI and PI
    fax.fill_between(data[xcol], data[upper_ci_col], data[lower_ci_col],
                     color="pink", alpha=0.5,
                     edgecolor="red", linestyle="dashed",
                     label="$95\%$ CI")
    fax.fill_between(data[xcol], data[upper_pi_col], data[lower_pi_col],
                     color="lightpink", alpha=0.5,
                     edgecolor="red", linestyle="dotted",
                     label="$95\%$ PI")

    fax.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)

    # residuals with mean and 1 sigma error band
    rax.set_ylabel("$"+ycol+"-\hat{y}$")
    rax.set_xlabel("$"+xcol+"$")
    rax.scatter(data[xcol], data[rescol],
                marker=".", color="black")
    res_mean = data[rescol].mean()
    res_std_dev = np.std(data[rescol])
    l = "$\\overline{res} = %.2f$\n$SS_{res} = %.2f$\n$SSP_{exp} = %.2f$\n$SS_{lack} = %.2f$\n$p_{LOF} = %.4f$"\
        %(res_mean, SSres, SSPexp, SSlack, pLOF)
    rax.hlines(res_mean, min(data[xcol]), max(data[xcol]),
               color="red", label=l)
    l = "$\\sigma_{res} = %.2f$" %res_std_dev
    rax.fill_between(data[xcol], res_mean+res_std_dev, res_mean-res_std_dev,
                     color="pink", alpha=0.5,
                     edgecolor="red", linestyle="dashed",
                     label=l)
    rax.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)

    return B1, B0, rsq, pLOF, fig
\end{lstlisting}

\begin{figure}[H]
    \includegraphics[width=0.49\textwidth]{lof_test_fail}
    \hfill
    \includegraphics[width=0.49\textwidth]{lof_test_pass}
    \caption{Sample output from the fit function. Because I saw some surprisingly large p-values for lack of fit tests in my dataset, I wanted to test my code on data with known results. On the left, I reproduced examples 11.10 and 11.11 in the class text. On the right, I tested perfectly linear data to ensure that the p-value responded as expected.}
\end{figure}

\newpage
\noindent\textbf{Appendix: Figures}
\begin{figure}[H]
    \includegraphics[width=0.5\textwidth]{Squarex_Squarey_binned}
    \includegraphics[width=0.5\textwidth]{x_y_binned}
    \includegraphics[width=0.5\textwidth]{Inversex_y_binned}
    \includegraphics[width=0.5\textwidth]{x_Inversey_binned}
    \caption{Selected results from testing every permutation with replacement of independent and dependent transformations. From top to bottom and left to right: square x vs square y, x vs y, inverse x vs y, and x vs inverse y.}
    \label{fig:perm}
\end{figure}

\end{document}